// Code generated by "core generate"; DO NOT EDIT.

package leabra

import (
	"cogentcore.org/core/gti"
)

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.ActParams", IDName: "act-params", Doc: "leabra.ActParams contains all the activation computation params and functions\nfor basic Leabra, at the neuron level .\nThis is included in leabra.Layer to drive the computation.", Fields: []gti.Field{{Name: "XX1", Doc: "Noisy X/X+1 rate code activation function parameters"}, {Name: "OptThresh", Doc: "optimization thresholds for faster processing"}, {Name: "Init", Doc: "initial values for key network state variables -- initialized at start of trial with InitActs or DecayActs"}, {Name: "Dt", Doc: "time and rate constants for temporal derivatives / updating of activation state"}, {Name: "Gbar", Doc: "maximal conductances levels for channels"}, {Name: "Erev", Doc: "reversal potentials for each channel"}, {Name: "Clamp", Doc: "how external inputs drive neural activations"}, {Name: "Noise", Doc: "how, where, when, and how much noise to add to activations"}, {Name: "VmRange", Doc: "range for Vm membrane potential -- by default"}, {Name: "KNa", Doc: "sodium-gated potassium channel adaptation parameters -- activates an inhibitory leak-like current as a function of neural activity (firing = Na influx) at three different time-scales (M-type = fast, Slick = medium, Slack = slow)"}, {Name: "ErevSubThr", Doc: "Erev - Act.Thr for each channel -- used in computing GeThrFmG among others"}, {Name: "ThrSubErev", Doc: "Act.Thr - Erev for each channel -- used in computing GeThrFmG among others"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.OptThreshParams", IDName: "opt-thresh-params", Doc: "OptThreshParams provides optimization thresholds for faster processing", Fields: []gti.Field{{Name: "Send", Doc: "don't send activation when act <= send -- greatly speeds processing"}, {Name: "Delta", Doc: "don't send activation changes until they exceed this threshold: only for when LeabraNetwork::send_delta is on!"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.ActInitParams", IDName: "act-init-params", Doc: "ActInitParams are initial values for key network state variables.\nInitialized at start of trial with Init_Acts or DecayState.", Fields: []gti.Field{{Name: "Decay", Doc: "proportion to decay activation state toward initial values at start of every trial"}, {Name: "Vm", Doc: "initial membrane potential -- see e_rev.l for the resting potential (typically .3) -- often works better to have a somewhat elevated initial membrane potential relative to that"}, {Name: "Act", Doc: "initial activation value -- typically 0"}, {Name: "Ge", Doc: "baseline level of excitatory conductance (net input) -- Ge is initialized to this value, and it is added in as a constant background level of excitatory input -- captures all the other inputs not represented in the model, and intrinsic excitability, etc"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.DtParams", IDName: "dt-params", Doc: "DtParams are time and rate constants for temporal derivatives in Leabra (Vm, net input)", Fields: []gti.Field{{Name: "Integ", Doc: "overall rate constant for numerical integration, for all equations at the unit level -- all time constants are specified in millisecond units, with one cycle = 1 msec -- if you instead want to make one cycle = 2 msec, you can do this globally by setting this integ value to 2 (etc).  However, stability issues will likely arise if you go too high.  For improved numerical stability, you may even need to reduce this value to 0.5 or possibly even lower (typically however this is not necessary).  MUST also coordinate this with network.time_inc variable to ensure that global network.time reflects simulated time accurately"}, {Name: "VmTau", Doc: "membrane potential and rate-code activation time constant in cycles, which should be milliseconds typically (roughly, how long it takes for value to change significantly -- 1.4x the half-life) -- reflects the capacitance of the neuron in principle -- biological default for AdEx spiking model C = 281 pF = 2.81 normalized -- for rate-code activation, this also determines how fast to integrate computed activation values over time"}, {Name: "GTau", Doc: "time constant for integrating synaptic conductances, in cycles, which should be milliseconds typically (roughly, how long it takes for value to change significantly -- 1.4x the half-life) -- this is important for damping oscillations -- generally reflects time constants associated with synaptic channels which are not modeled in the most abstract rate code models (set to 1 for detailed spiking models with more realistic synaptic currents) -- larger values (e.g., 3) can be important for models with higher conductances that otherwise might be more prone to oscillation."}, {Name: "AvgTau", Doc: "for integrating activation average (ActAvg), time constant in trials (roughly, how long it takes for value to change significantly) -- used mostly for visualization and tracking *hog* units"}, {Name: "VmDt", Doc: "nominal rate = Integ / tau"}, {Name: "GDt", Doc: "rate = Integ / tau"}, {Name: "AvgDt", Doc: "rate = 1 / tau"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.ActNoiseType", IDName: "act-noise-type", Doc: "ActNoiseType are different types / locations of random noise for activations"})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.ActNoiseParams", IDName: "act-noise-params", Doc: "ActNoiseParams contains parameters for activation-level noise", Embeds: []gti.Field{{Name: "RndParams"}}, Fields: []gti.Field{{Name: "Type", Doc: "where and how to add processing noise"}, {Name: "Fixed", Doc: "keep the same noise value over the entire alpha cycle -- prevents noise from being washed out and produces a stable effect that can be better used for learning -- this is strongly recommended for most learning situations"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.ClampParams", IDName: "clamp-params", Doc: "ClampParams are for specifying how external inputs are clamped onto network activation values", Fields: []gti.Field{{Name: "Hard", Doc: "whether to hard clamp inputs where activation is directly set to external input value (Act = Ext) or do soft clamping where Ext is added into Ge excitatory current (Ge += Gain * Ext)"}, {Name: "Range", Doc: "range of external input activation values allowed -- Max is .95 by default due to saturating nature of rate code activation function"}, {Name: "Gain", Doc: "soft clamp gain factor (Ge += Gain * Ext)"}, {Name: "Avg", Doc: "compute soft clamp as the average of current and target netins, not the sum -- prevents some of the main effect problems associated with adding external inputs"}, {Name: "AvgGain", Doc: "gain factor for averaging the Ge -- clamp value Ext contributes with AvgGain and current Ge as (1-AvgGain)"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.WtInitParams", IDName: "wt-init-params", Doc: "WtInitParams are weight initialization parameters -- basically the\nrandom distribution parameters but also Symmetry flag", Embeds: []gti.Field{{Name: "RndParams"}}, Fields: []gti.Field{{Name: "Sym", Doc: "symmetrize the weight values with those in reciprocal projection -- typically true for bidirectional excitatory connections"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.WtScaleParams", IDName: "wt-scale-params", Doc: "/ WtScaleParams are weight scaling parameters: modulates overall strength of projection,\nusing both absolute and relative factors", Fields: []gti.Field{{Name: "Abs", Doc: "absolute scaling, which is not subject to normalization: directly multiplies weight values"}, {Name: "Rel", Doc: "relative scaling that shifts balance between different projections -- this is subject to normalization across all other projections into unit"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.InhibParams", IDName: "inhib-params", Doc: "leabra.InhibParams contains all the inhibition computation params and functions for basic Leabra\nThis is included in leabra.Layer to support computation.\nThis also includes other misc layer-level params such as running-average activation in the layer\nwhich is used for netinput rescaling and potentially for adapting inhibition over time", Fields: []gti.Field{{Name: "Layer", Doc: "inhibition across the entire layer"}, {Name: "Pool", Doc: "inhibition across sub-pools of units, for layers with 4D shape"}, {Name: "Self", Doc: "neuron self-inhibition parameters -- can be beneficial for producing more graded, linear response -- not typically used in cortical networks"}, {Name: "ActAvg", Doc: "running-average activation computation values -- for overall estimates of layer activation levels, used in netinput scaling"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.SelfInhibParams", IDName: "self-inhib-params", Doc: "SelfInhibParams defines parameters for Neuron self-inhibition -- activation of the neuron directly feeds back\nto produce a proportional additional contribution to Gi", Fields: []gti.Field{{Name: "On", Doc: "enable neuron self-inhibition"}, {Name: "Gi", Doc: "strength of individual neuron self feedback inhibition -- can produce proportional activation behavior in individual units for specialized cases (e.g., scalar val or BG units), but not so good for typical hidden layers"}, {Name: "Tau", Doc: "time constant in cycles, which should be milliseconds typically (roughly, how long it takes for value to change significantly -- 1.4x the half-life) for integrating unit self feedback inhibitory values -- prevents oscillations that otherwise occur -- relatively rapid 1.4 typically works, but may need to go longer if oscillations are a problem"}, {Name: "Dt", Doc: "rate = 1 / tau"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.ActAvgParams", IDName: "act-avg-params", Doc: "ActAvgParams represents expected average activity levels in the layer.\nUsed for computing running-average computation that is then used for netinput scaling.\nAlso specifies time constant for updating average\nand for the target value for adapting inhibition in inhib_adapt.", Fields: []gti.Field{{Name: "Init", Doc: "initial estimated average activity level in the layer (see also UseFirst option -- if that is off then it is used as a starting point for running average actual activity level, ActMAvg and ActPAvg) -- ActPAvg is used primarily for automatic netinput scaling, to balance out layers that have different activity levels -- thus it is important that init be relatively accurate -- good idea to update from recorded ActPAvg levels"}, {Name: "Fixed", Doc: "if true, then the Init value is used as a constant for ActPAvgEff (the effective value used for netinput rescaling), instead of using the actual running average activation"}, {Name: "UseExtAct", Doc: "if true, then use the activation level computed from the external inputs to this layer (avg of targ or ext unit vars) -- this will only be applied to layers with Input or Target / Compare layer types, and falls back on the targ_init value if external inputs are not available or have a zero average -- implies fixed behavior"}, {Name: "UseFirst", Doc: "use the first actual average value to override targ_init value -- actual value is likely to be a better estimate than our guess"}, {Name: "Tau", Doc: "time constant in trials for integrating time-average values at the layer level -- used for computing Pool.ActAvg.ActsMAvg, ActsPAvg"}, {Name: "Adjust", Doc: "adjustment multiplier on the computed ActPAvg value that is used to compute ActPAvgEff, which is actually used for netinput rescaling -- if based on connectivity patterns or other factors the actual running-average value is resulting in netinputs that are too high or low, then this can be used to adjust the effective average activity value -- reducing the average activity with a factor < 1 will increase netinput scaling (stronger net inputs from layers that receive from this layer), and vice-versa for increasing (decreases net inputs)"}, {Name: "Dt", Doc: "rate = 1 / tau"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.Layer", IDName: "layer", Doc: "leabra.Layer has parameters for running a basic rate-coded Leabra layer", Embeds: []gti.Field{{Name: "LayerBase"}}, Fields: []gti.Field{{Name: "Act", Doc: "Activation parameters and methods for computing activations"}, {Name: "Inhib", Doc: "Inhibition parameters and methods for computing layer-level inhibition"}, {Name: "Learn", Doc: "Learning parameters and methods that operate at the neuron level"}, {Name: "Neurons", Doc: "slice of neurons for this layer -- flat list of len = Shp.Len(). You must iterate over index and use pointer to modify values."}, {Name: "Pools", Doc: "inhibition and other pooled, aggregate state variables -- flat list has at least of 1 for layer, and one for each sub-pool (unit group) if shape supports that (4D).  You must iterate over index and use pointer to modify values."}, {Name: "CosDiff", Doc: "cosine difference between ActM, ActP stats"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.LayerBase", IDName: "layer-base", Doc: "LayerBase manages the structural elements of the layer, which are common\nto any Layer type", Fields: []gti.Field{{Name: "LeabraLay", Doc: "we need a pointer to ourselves as an LeabraLayer (which subsumes emer.Layer), which can always be used to extract the true underlying type of object when layer is embedded in other structs -- function receivers do not have this ability so this is necessary."}, {Name: "Network", Doc: "our parent network, in case we need to use it to find other layers etc -- set when added by network"}, {Name: "Nm", Doc: "Name of the layer -- this must be unique within the network, which has a map for quick lookup and layers are typically accessed directly by name"}, {Name: "Cls", Doc: "Class is for applying parameter styles, can be space separated multple tags"}, {Name: "Off", Doc: "inactivate this layer -- allows for easy experimentation"}, {Name: "Shp", Doc: "shape of the layer -- can be 2D for basic layers and 4D for layers with sub-groups (hypercolumns) -- order is outer-to-inner (row major) so Y then X for 2D and for 4D: Y-X unit pools then Y-X neurons within pools"}, {Name: "Typ", Doc: "type of layer -- Hidden, Input, Target, Compare, or extended type in specialized algorithms -- matches against .Class parameter styles (e.g., .Hidden etc)"}, {Name: "Thr", Doc: "the thread number (go routine) to use in updating this layer. The user is responsible for allocating layers to threads, trying to maintain an even distribution across layers and establishing good break-points."}, {Name: "Rel", Doc: "Spatial relationship to other layer, determines positioning"}, {Name: "Ps", Doc: "position of lower-left-hand corner of layer in 3D space, computed from Rel.  Layers are in X-Y width - height planes, stacked vertically in Z axis."}, {Name: "Index", Doc: "a 0..n-1 index of the position of the layer within list of layers in the network. For Leabra networks, it only has significance in determining who gets which weights for enforcing initial weight symmetry -- higher layers get weights from lower layers."}, {Name: "RepIxs", Doc: "indexes of representative units in the layer, for computationally expensive stats or displays"}, {Name: "RepShp", Doc: "shape of representative units in the layer -- if RepIxs is empty or .Shp is nil, use overall layer shape"}, {Name: "RcvPrjns", Doc: "list of receiving projections into this layer from other layers"}, {Name: "SndPrjns", Doc: "list of sending projections from this layer to other layers"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.LeabraNetwork", IDName: "leabra-network", Doc: "LeabraNetwork defines the essential algorithmic API for Leabra, at the network level.\nThese are the methods that the user calls in their Sim code:\n* AlphaCycInit\n* Cycle\n* QuarterFinal\n* DWt\n* WtFmDwt\nBecause we don't want to have to force the user to use the interface cast in calling\nthese methods, we provide Impl versions here that are the implementations\nwhich the user-facing method calls.\n\nTypically most changes in algorithm can be accomplished directly in the Layer\nor Prjn level, but sometimes (e.g., in deep) additional full-network passes\nare required.\n\nAll of the structural API is in emer.Network, which this interface also inherits for\nconvenience."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.LeabraLayer", IDName: "leabra-layer", Doc: "LeabraLayer defines the essential algorithmic API for Leabra, at the layer level.\nThese are the methods that the leabra.Network calls on its layers at each step\nof processing.  Other Layer types can selectively re-implement (override) these methods\nto modify the computation, while inheriting the basic behavior for non-overridden methods.\n\nAll of the structural API is in emer.Layer, which this interface also inherits for\nconvenience."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.LeabraPrjn", IDName: "leabra-prjn", Doc: "LeabraPrjn defines the essential algorithmic API for Leabra, at the projection level.\nThese are the methods that the leabra.Layer calls on its prjns at each step\nof processing.  Other Prjn types can selectively re-implement (override) these methods\nto modify the computation, while inheriting the basic behavior for non-overridden methods.\n\nAll of the structural API is in emer.Prjn, which this interface also inherits for\nconvenience."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.LeabraPrjns", IDName: "leabra-prjns"})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.LearnNeurParams", IDName: "learn-neur-params", Doc: "leabra.LearnNeurParams manages learning-related parameters at the neuron-level.\nThis is mainly the running average activations that drive learning", Fields: []gti.Field{{Name: "ActAvg", Doc: "parameters for computing running average activations that drive learning"}, {Name: "AvgL", Doc: "parameters for computing AvgL long-term running average"}, {Name: "CosDiff", Doc: "parameters for computing cosine diff between minus and plus phase"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.LearnSynParams", IDName: "learn-syn-params", Doc: "leabra.LearnSynParams manages learning-related parameters at the synapse-level.", Fields: []gti.Field{{Name: "Learn", Doc: "enable learning for this projection"}, {Name: "Lrate", Doc: "current effective learning rate (multiplies DWt values, determining rate of change of weights)"}, {Name: "LrateInit", Doc: "initial learning rate -- this is set from Lrate in UpdateParams, which is called when Params are updated, and used in LrateMult to compute a new learning rate for learning rate schedules."}, {Name: "XCal", Doc: "parameters for the XCal learning rule"}, {Name: "WtSig", Doc: "parameters for the sigmoidal contrast weight enhancement"}, {Name: "Norm", Doc: "parameters for normalizing weight changes by abs max dwt"}, {Name: "Momentum", Doc: "parameters for momentum across weight changes"}, {Name: "WtBal", Doc: "parameters for balancing strength of weight increases vs. decreases"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.LrnActAvgParams", IDName: "lrn-act-avg-params", Doc: "LrnActAvgParams has rate constants for averaging over activations at different time scales,\nto produce the running average activation values that then drive learning in the XCAL learning rules", Fields: []gti.Field{{Name: "SSTau", Doc: "time constant in cycles, which should be milliseconds typically (roughly, how long it takes for value to change significantly -- 1.4x the half-life), for continuously updating the super-short time-scale avg_ss value -- this is provides a pre-integration step before integrating into the avg_s short time scale -- it is particularly important for spiking -- in general 4 is the largest value without starting to impair learning, but a value of 7 can be combined with m_in_s = 0 with somewhat worse results"}, {Name: "STau", Doc: "time constant in cycles, which should be milliseconds typically (roughly, how long it takes for value to change significantly -- 1.4x the half-life), for continuously updating the short time-scale avg_s value from the super-short avg_ss value (cascade mode) -- avg_s represents the plus phase learning signal that reflects the most recent past information"}, {Name: "MTau", Doc: "time constant in cycles, which should be milliseconds typically (roughly, how long it takes for value to change significantly -- 1.4x the half-life), for continuously updating the medium time-scale avg_m value from the short avg_s value (cascade mode) -- avg_m represents the minus phase learning signal that reflects the expectation representation prior to experiencing the outcome (in addition to the outcome) -- the default value of 10 generally cannot be exceeded without impairing learning"}, {Name: "LrnM", Doc: "how much of the medium term average activation to mix in with the short (plus phase) to compute the Neuron AvgSLrn variable that is used for the unit's short-term average in learning. This is important to ensure that when unit turns off in plus phase (short time scale), enough medium-phase trace remains so that learning signal doesn't just go all the way to 0, at which point no learning would take place -- typically need faster time constant for updating S such that this trace of the M signal is lost -- can set SSTau=7 and set this to 0 but learning is generally somewhat worse"}, {Name: "Init", Doc: "initial value for average"}, {Name: "SSDt", Doc: "rate = 1 / tau"}, {Name: "SDt", Doc: "rate = 1 / tau"}, {Name: "MDt", Doc: "rate = 1 / tau"}, {Name: "LrnS", Doc: "1-LrnM"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.AvgLParams", IDName: "avg-l-params", Doc: "AvgLParams are parameters for computing the long-term floating average value, AvgL\nwhich is used for driving BCM-style hebbian learning in XCAL -- this form of learning\nincreases contrast of weights and generally decreases overall activity of neuron,\nto prevent \"hog\" units -- it is computed as a running average of the (gain multiplied)\nmedium-time-scale average activation at the end of the alpha-cycle.\nAlso computes an adaptive amount of BCM learning, AvgLLrn, based on AvgL.", Fields: []gti.Field{{Name: "Init", Doc: "initial AvgL value at start of training"}, {Name: "Gain", Doc: "gain multiplier on activation used in computing the running average AvgL value that is the key floating threshold in the BCM Hebbian learning rule -- when using the DELTA_FF_FB learning rule, it should generally be 2x what it was before with the old XCAL_CHL rule, i.e., default of 5 instead of 2.5 -- it is a good idea to experiment with this parameter a bit -- the default is on the high-side, so typically reducing a bit from initial default is a good direction"}, {Name: "Min", Doc: "miniumum AvgL value -- running average cannot go lower than this value even when it otherwise would due to inactivity -- default value is generally good and typically does not need to be changed"}, {Name: "Tau", Doc: "time constant for updating the running average AvgL -- AvgL moves toward gain*act with this time constant on every alpha-cycle - longer time constants can also work fine, but the default of 10 allows for quicker reaction to beneficial weight changes"}, {Name: "LrnMax", Doc: "maximum AvgLLrn value, which is amount of learning driven by AvgL factor -- when AvgL is at its maximum value (i.e., gain, as act does not exceed 1), then AvgLLrn will be at this maximum value -- by default, strong amounts of this homeostatic Hebbian form of learning can be used when the receiving unit is highly active -- this will then tend to bring down the average activity of units -- the default of 0.5, in combination with the err_mod flag, works well for most models -- use around 0.0004 for a single fixed value (with err_mod flag off)"}, {Name: "LrnMin", Doc: "miniumum AvgLLrn value (amount of learning driven by AvgL factor) -- if AvgL is at its minimum value, then AvgLLrn will be at this minimum value -- neurons that are not overly active may not need to increase the contrast of their weights as much -- use around 0.0004 for a single fixed value (with err_mod flag off)"}, {Name: "ErrMod", Doc: "modulate amount learning by normalized level of error within layer"}, {Name: "ModMin", Doc: "minimum modulation value for ErrMod-- ensures a minimum amount of self-organizing learning even for network / layers that have a very small level of error signal"}, {Name: "Dt", Doc: "rate = 1 / tau"}, {Name: "LrnFact", Doc: "(LrnMax - LrnMin) / (Gain - Min)"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.CosDiffParams", IDName: "cos-diff-params", Doc: "CosDiffParams specify how to integrate cosine of difference between plus and minus phase activations\nUsed to modulate amount of hebbian learning, and overall learning rate.", Fields: []gti.Field{{Name: "Tau", Doc: "time constant in alpha-cycles (roughly how long significant change takes, 1.4 x half-life) for computing running average CosDiff value for the layer, CosDiffAvg = cosine difference between ActM and ActP -- this is an important statistic for how much phase-based difference there is between phases in this layer -- it is used in standard X_COS_DIFF modulation of l_mix in LeabraConSpec, and for modulating learning rate as a function of predictability in the DeepLeabra predictive auto-encoder learning -- running average variance also computed with this: cos_diff_var"}, {Name: "Dt", Doc: "rate constant = 1 / Tau"}, {Name: "DtC", Doc: "complement of rate constant = 1 - Dt"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.CosDiffStats", IDName: "cos-diff-stats", Doc: "CosDiffStats holds cosine-difference statistics at the layer level", Fields: []gti.Field{{Name: "Cos", Doc: "cosine (normalized dot product) activation difference between ActP and ActM on this alpha-cycle for this layer -- computed by CosDiffFmActs at end of QuarterFinal for quarter = 3"}, {Name: "Avg", Doc: "running average of cosine (normalized dot product) difference between ActP and ActM -- computed with CosDiff.Tau time constant in QuarterFinal, and used for modulating BCM Hebbian learning (see AvgLrn) and overall learning rate"}, {Name: "Var", Doc: "running variance of cosine (normalized dot product) difference between ActP and ActM -- computed with CosDiff.Tau time constant in QuarterFinal, used for modulating overall learning rate"}, {Name: "AvgLrn", Doc: "1 - Avg and 0 for non-Hidden layers"}, {Name: "ModAvgLLrn", Doc: "1 - AvgLrn and 0 for non-Hidden layers -- this is the value of Avg used for AvgLParams ErrMod modulation of the AvgLLrn factor if enabled"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.XCalParams", IDName: "x-cal-params", Doc: "XCalParams are parameters for temporally eXtended Contrastive Attractor Learning function (XCAL)\nwhich is the standard learning equation for leabra .", Fields: []gti.Field{{Name: "MLrn", Doc: "multiplier on learning based on the medium-term floating average threshold which produces error-driven learning -- this is typically 1 when error-driven learning is being used, and 0 when pure Hebbian learning is used. The long-term floating average threshold is provided by the receiving unit"}, {Name: "SetLLrn", Doc: "if true, set a fixed AvgLLrn weighting factor that determines how much of the long-term floating average threshold (i.e., BCM, Hebbian) component of learning is used -- this is useful for setting a fully Hebbian learning connection, e.g., by setting MLrn = 0 and LLrn = 1. If false, then the receiving unit's AvgLLrn factor is used, which dynamically modulates the amount of the long-term component as a function of how active overall it is"}, {Name: "LLrn", Doc: "fixed l_lrn weighting factor that determines how much of the long-term floating average threshold (i.e., BCM, Hebbian) component of learning is used -- this is useful for setting a fully Hebbian learning connection, e.g., by setting MLrn = 0 and LLrn = 1."}, {Name: "DRev", Doc: "proportional point within LTD range where magnitude reverses to go back down to zero at zero -- err-driven svm component does better with smaller values, and BCM-like mvl component does better with larger values -- 0.1 is a compromise"}, {Name: "DThr", Doc: "minimum LTD threshold value below which no weight change occurs -- this is now *relative* to the threshold"}, {Name: "LrnThr", Doc: "xcal learning threshold -- don't learn when sending unit activation is below this value in both phases -- due to the nature of the learning function being 0 when the sr coproduct is 0, it should not affect learning in any substantial way -- nonstandard learning algorithms that have different properties should ignore it"}, {Name: "DRevRatio", Doc: "-(1-DRev)/DRev -- multiplication factor in learning rule -- builds in the minus sign!"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.WtSigParams", IDName: "wt-sig-params", Doc: "WtSigParams are sigmoidal weight contrast enhancement function parameters", Fields: []gti.Field{{Name: "Gain", Doc: "gain (contrast, sharpness) of the weight contrast function (1 = linear)"}, {Name: "Off", Doc: "offset of the function (1=centered at .5, >1=higher, <1=lower) -- 1 is standard for XCAL"}, {Name: "SoftBound", Doc: "apply exponential soft bounding to the weight changes"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.DWtNormParams", IDName: "d-wt-norm-params", Doc: "DWtNormParams are weight change (dwt) normalization parameters, using MAX(ABS(dwt)) aggregated over\nSending connections in a given projection for a given unit.\nSlowly decays and instantly resets to any current max(abs)\nServes as an estimate of the variance in the weight changes, assuming zero net mean overall.", Fields: []gti.Field{{Name: "On", Doc: "whether to use dwt normalization, only on error-driven dwt component, based on projection-level max_avg value -- slowly decays and instantly resets to any current max"}, {Name: "DecayTau", Doc: "time constant for decay of dwnorm factor -- generally should be long-ish, between 1000-10000 -- integration rate factor is 1/tau"}, {Name: "NormMin", Doc: "minimum effective value of the normalization factor -- provides a lower bound to how much normalization can be applied"}, {Name: "LrComp", Doc: "overall learning rate multiplier to compensate for changes due to use of normalization -- allows for a common master learning rate to be used between different conditions -- 0.1 for synapse-level, maybe higher for other levels"}, {Name: "Stats", Doc: "record the avg, max values of err, bcm hebbian, and overall dwt change per con group and per projection"}, {Name: "DecayDt", Doc: "rate constant of decay = 1 / decay_tau"}, {Name: "DecayDtC", Doc: "complement rate constant of decay = 1 - (1 / decay_tau)"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.MomentumParams", IDName: "momentum-params", Doc: "MomentumParams implements standard simple momentum -- accentuates consistent directions of weight change and\ncancels out dithering -- biologically captures slower timecourse of longer-term plasticity mechanisms.", Fields: []gti.Field{{Name: "On", Doc: "whether to use standard simple momentum"}, {Name: "MTau", Doc: "time constant factor for integration of momentum -- 1/tau is dt (e.g., .1), and 1-1/tau (e.g., .95 or .9) is traditional momentum time-integration factor"}, {Name: "LrComp", Doc: "overall learning rate multiplier to compensate for changes due to JUST momentum without normalization -- allows for a common master learning rate to be used between different conditions -- generally should use .1 to compensate for just momentum itself"}, {Name: "MDt", Doc: "rate constant of momentum integration = 1 / m_tau"}, {Name: "MDtC", Doc: "complement rate constant of momentum integration = 1 - (1 / m_tau)"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.WtBalParams", IDName: "wt-bal-params", Doc: "WtBalParams are weight balance soft renormalization params:\nmaintains overall weight balance by progressively penalizing weight increases as a function of\nhow strong the weights are overall (subject to thresholding) and long time-averaged activation.\nPlugs into soft bounding function.", Fields: []gti.Field{{Name: "On", Doc: "perform weight balance soft normalization?  if so, maintains overall weight balance across units by progressively penalizing weight increases as a function of amount of averaged receiver weight above a high threshold (hi_thr) and long time-average activation above an act_thr -- this is generally very beneficial for larger models where hog units are a problem, but not as much for smaller models where the additional constraints are not beneficial -- uses a sigmoidal function: WbInc = 1 / (1 + HiGain*(WbAvg - HiThr) + ActGain * (nrn.ActAvg - ActThr)))"}, {Name: "Targs", Doc: "apply soft bounding to target layers -- appears to be beneficial but still testing"}, {Name: "AvgThr", Doc: "threshold on weight value for inclusion into the weight average that is then subject to the further HiThr threshold for then driving a change in weight balance -- this AvgThr allows only stronger weights to contribute so that weakening of lower weights does not dilute sensitivity to number and strength of strong weights"}, {Name: "HiThr", Doc: "high threshold on weight average (subject to AvgThr) before it drives changes in weight increase vs. decrease factors"}, {Name: "HiGain", Doc: "gain multiplier applied to above-HiThr thresholded weight averages -- higher values turn weight increases down more rapidly as the weights become more imbalanced"}, {Name: "LoThr", Doc: "low threshold on weight average (subject to AvgThr) before it drives changes in weight increase vs. decrease factors"}, {Name: "LoGain", Doc: "gain multiplier applied to below-lo_thr thresholded weight averages -- higher values turn weight increases up more rapidly as the weights become more imbalanced -- generally beneficial but sometimes not -- worth experimenting with either 6 or 0"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.Network", IDName: "network", Doc: "Network has parameters for running a basic rate-coded Leabra network", Embeds: []gti.Field{{Name: "NetworkBase"}}, Fields: []gti.Field{{Name: "WtBalInterval", Doc: "how frequently to update the weight balance average weight factor -- relatively expensive"}, {Name: "WtBalCtr", Doc: "counter for how long it has been since last WtBal"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.td", IDName: "td", Fields: []gti.Field{{Name: "Lays"}, {Name: "Neur"}, {Name: "Syn"}, {Name: "Tot"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.LayFunChan", IDName: "lay-fun-chan", Doc: "LayFunChan is a channel that runs LeabraLayer functions"})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.NetworkBase", IDName: "network-base", Doc: "leabra.NetworkBase holds the basic structural components of a network (layers)", Fields: []gti.Field{{Name: "EmerNet", Doc: "we need a pointer to ourselves as an emer.Network, which can always be used to extract the true underlying type of object when network is embedded in other structs -- function receivers do not have this ability so this is necessary."}, {Name: "Nm", Doc: "overall name of network -- helps discriminate if there are multiple"}, {Name: "Layers", Doc: "list of layers"}, {Name: "WtsFile", Doc: "filename of last weights file loaded or saved"}, {Name: "LayMap", Doc: "map of name to layers -- layer names must be unique"}, {Name: "LayClassMap", Doc: "map of layer classes -- made during Build"}, {Name: "MinPos", Doc: "minimum display position in network"}, {Name: "MaxPos", Doc: "maximum display position in network"}, {Name: "MetaData", Doc: "optional metadata that is saved in network weights files -- e.g., can indicate number of epochs that were trained, or any other information about this network that would be useful to save"}, {Name: "NThreads", Doc: "number of parallel threads (go routines) to use -- this is computed directly from the Layers which you must explicitly allocate to different threads -- updated during Build of network"}, {Name: "LockThreads", Doc: "if set, runtime.LockOSThread() is called on the compute threads, which can be faster on large networks on some architectures -- experimentation is recommended"}, {Name: "ThrLay", Doc: "layers per thread -- outer group is threads and inner is layers operated on by that thread -- based on user-assigned threads, initialized during Build"}, {Name: "ThrChans", Doc: "layer function channels, per thread"}, {Name: "ThrTimes", Doc: "timers for each thread, so you can see how evenly the workload is being distributed"}, {Name: "FunTimes", Doc: "timers for each major function (step of processing)"}, {Name: "WaitGp", Doc: "network-level wait group for synchronizing threaded layer calls"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.Neuron", IDName: "neuron", Doc: "leabra.Neuron holds all of the neuron (unit) level variables -- this is the most basic version with\nrate-code only and no optional features at all.\nAll variables accessible via Unit interface must be float32 and start at the top, in contiguous order", Fields: []gti.Field{{Name: "Flags", Doc: "bit flags for binary state variables"}, {Name: "SubPool", Doc: "index of the sub-level inhibitory pool that this neuron is in (only for 4D shapes, the pool (unit-group / hypercolumn) structure level) -- indicies start at 1 -- 0 is layer-level pool (is 0 if no sub-pools)."}, {Name: "Act", Doc: "rate-coded activation value reflecting final output of neuron communicated to other neurons, typically in range 0-1.  This value includes adaptation and synaptic depression / facilitation effects which produce temporal contrast (see ActLrn for version without this).  For rate-code activation, this is noisy-x-over-x-plus-one (NXX1) function; for discrete spiking it is computed from the inverse of the inter-spike interval (ISI), and Spike reflects the discrete spikes."}, {Name: "ActLrn", Doc: "learning activation value, reflecting *dendritic* activity that is not affected by synaptic depression or adapdation channels which are located near the axon hillock.  This is the what drives the Avg* values that drive learning. Computationally, neurons strongly discount the signals sent to other neurons to provide temporal contrast, but need to learn based on a more stable reflection of their overall inputs in the dendrites."}, {Name: "Ge", Doc: "total excitatory synaptic conductance -- the net excitatory input to the neuron -- does *not* include Gbar.E"}, {Name: "Gi", Doc: "total inhibitory synaptic conductance -- the net inhibitory input to the neuron -- does *not* include Gbar.I"}, {Name: "Gk", Doc: "total potassium conductance, typically reflecting sodium-gated potassium currents involved in adaptation effects -- does *not* include Gbar.K"}, {Name: "Inet", Doc: "net current produced by all channels -- drives update of Vm"}, {Name: "Vm", Doc: "membrane potential -- integrates Inet current over time"}, {Name: "Targ", Doc: "target value: drives learning to produce this activation value"}, {Name: "Ext", Doc: "external input: drives activation of unit from outside influences (e.g., sensory input)"}, {Name: "AvgSS", Doc: "super-short time-scale average of ActLrn activation -- provides the lowest-level time integration -- for spiking this integrates over spikes before subsequent averaging, and it is also useful for rate-code to provide a longer time integral overall"}, {Name: "AvgS", Doc: "short time-scale average of ActLrn activation -- tracks the most recent activation states (integrates over AvgSS values), and represents the plus phase for learning in XCAL algorithms"}, {Name: "AvgM", Doc: "medium time-scale average of ActLrn activation -- integrates over AvgS values, and represents the minus phase for learning in XCAL algorithms"}, {Name: "AvgL", Doc: "long time-scale average of medium-time scale (trial level) activation, used for the BCM-style floating threshold in XCAL"}, {Name: "AvgLLrn", Doc: "how much to learn based on the long-term floating threshold (AvgL) for BCM-style Hebbian learning -- is modulated by level of AvgL itself (stronger Hebbian as average activation goes higher) and optionally the average amount of error experienced in the layer (to retain a common proportionality with the level of error-driven learning across layers)"}, {Name: "AvgSLrn", Doc: "short time-scale activation average that is actually used for learning -- typically includes a small contribution from AvgM in addition to mostly AvgS, as determined by LrnActAvgParams.LrnM -- important to ensure that when unit turns off in plus phase (short time scale), enough medium-phase trace remains so that learning signal doesn't just go all the way to 0, at which point no learning would take place"}, {Name: "ActQ0", Doc: "the activation state at start of current alpha cycle (same as the state at end of previous cycle)"}, {Name: "ActQ1", Doc: "the activation state at end of first quarter of current alpha cycle"}, {Name: "ActQ2", Doc: "the activation state at end of second quarter of current alpha cycle"}, {Name: "ActM", Doc: "the activation state at end of third quarter, which is the traditional posterior-cortical minus phase activation"}, {Name: "ActP", Doc: "the activation state at end of fourth quarter, which is the traditional posterior-cortical plus_phase activation"}, {Name: "ActDif", Doc: "ActP - ActM -- difference between plus and minus phase acts -- reflects the individual error gradient for this neuron in standard error-driven learning terms"}, {Name: "ActDel", Doc: "delta activation: change in Act from one cycle to next -- can be useful to track where changes are taking place"}, {Name: "ActAvg", Doc: "average activation (of final plus phase activation state) over long time intervals (time constant = DtPars.AvgTau -- typically 200) -- useful for finding hog units and seeing overall distribution of activation"}, {Name: "Noise", Doc: "noise value added to unit (ActNoiseParams determines distribution, and when / where it is added)"}, {Name: "GiSyn", Doc: "aggregated synaptic inhibition (from Inhib projections) -- time integral of GiRaw -- this is added with computed FFFB inhibition to get the full inhibition in Gi"}, {Name: "GiSelf", Doc: "total amount of self-inhibition -- time-integrated to avoid oscillations"}, {Name: "ActSent", Doc: "last activation value sent (only send when diff is over threshold)"}, {Name: "GeRaw", Doc: "raw excitatory conductance (net input) received from sending units (send delta's are added to this value)"}, {Name: "GiRaw", Doc: "raw inhibitory conductance (net input) received from sending units (send delta's are added to this value)"}, {Name: "GknaFast", Doc: "conductance of sodium-gated potassium channel (KNa) fast dynamics (M-type) -- produces accommodation / adaptation of firing"}, {Name: "GknaMed", Doc: "conductance of sodium-gated potassium channel (KNa) medium dynamics (Slick) -- produces accommodation / adaptation of firing"}, {Name: "GknaSlow", Doc: "conductance of sodium-gated potassium channel (KNa) slow dynamics (Slack) -- produces accommodation / adaptation of firing"}, {Name: "Spike", Doc: "whether neuron has spiked or not (0 or 1), for discrete spiking neurons."}, {Name: "ISI", Doc: "current inter-spike-interval -- counts up since last spike.  Starts at -1 when initialized."}, {Name: "ISIAvg", Doc: "average inter-spike-interval -- average time interval between spikes.  Starts at -1 when initialized, and goes to -2 after first spike, and is only valid after the second spike post-initialization."}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.NeurFlags", IDName: "neur-flags", Doc: "NeurFlags are bit-flags encoding relevant binary state for neurons"})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.Pool", IDName: "pool", Doc: "Pool contains computed values for FFFB inhibition, and various other state values for layers\nand pools (unit groups) that can be subject to inhibition, including:\n* average / max stats on Ge and Act that drive inhibition\n* average activity overall that is used for normalizing netin (at layer level)", Fields: []gti.Field{{Name: "StIndex", Doc: "starting and ending (exlusive) indexes for the list of neurons in this pool"}, {Name: "EdIndex", Doc: "starting and ending (exlusive) indexes for the list of neurons in this pool"}, {Name: "Inhib", Doc: "FFFB inhibition computed values, including Ge and Act AvgMax which drive inhibition"}, {Name: "ActM", Doc: "minus phase average and max Act activation values, for ActAvg updt"}, {Name: "ActP", Doc: "plus phase average and max Act activation values, for ActAvg updt"}, {Name: "ActAvg", Doc: "running-average activation levels used for netinput scaling and adaptive inhibition"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.ActAvg", IDName: "act-avg", Doc: "ActAvg are running-average activation levels used for netinput scaling and adaptive inhibition", Fields: []gti.Field{{Name: "ActMAvg", Doc: "running-average minus-phase activity -- used for adapting inhibition -- see ActAvgParams.Tau for time constant etc"}, {Name: "ActPAvg", Doc: "running-average plus-phase activity -- used for synaptic input scaling -- see ActAvgParams.Tau for time constant etc"}, {Name: "ActPAvgEff", Doc: "ActPAvg * ActAvgParams.Adjust -- adjusted effective layer activity directly used in synaptic input scaling"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.Prjn", IDName: "prjn", Doc: "Prjn is a basic Leabra projection with synaptic learning parameters", Embeds: []gti.Field{{Name: "PrjnBase"}}, Fields: []gti.Field{{Name: "WtInit", Doc: "initial random weight distribution"}, {Name: "WtScale", Doc: "weight scaling parameters: modulates overall strength of projection, using both absolute and relative factors"}, {Name: "Learn", Doc: "synaptic-level learning parameters"}, {Name: "Syns", Doc: "synaptic state values, ordered by the sending layer units which owns them -- one-to-one with SConIndex array"}, {Name: "GScale", Doc: "scaling factor for integrating synaptic input conductances (G's) -- computed in AlphaCycInit, incorporates running-average activity levels"}, {Name: "GInc", Doc: "local per-recv unit increment accumulator for synaptic conductance from sending units -- goes to either GeRaw or GiRaw on neuron depending on projection type -- this will be thread-safe"}, {Name: "WbRecv", Doc: "weight balance state variables for this projection, one per recv neuron"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.WtBalRecvPrjn", IDName: "wt-bal-recv-prjn", Doc: "WtBalRecvPrjn are state variables used in computing the WtBal weight balance function\nThere is one of these for each Recv Neuron participating in the projection.", Fields: []gti.Field{{Name: "Avg", Doc: "average of effective weight values that exceed WtBal.AvgThr across given Recv Neuron's connections for given Prjn"}, {Name: "Fact", Doc: "overall weight balance factor that drives changes in WbInc vs. WbDec via a sigmoidal function -- this is the net strength of weight balance changes"}, {Name: "Inc", Doc: "weight balance increment factor -- extra multiplier to add to weight increases to maintain overall weight balance"}, {Name: "Dec", Doc: "weight balance decrement factor -- extra multiplier to add to weight decreases to maintain overall weight balance"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.PrjnBase", IDName: "prjn-base", Doc: "PrjnBase contains the basic structural information for specifying a projection of synaptic\nconnections between two layers, and maintaining all the synaptic connection-level data.\nThe exact same struct object is added to the Recv and Send layers, and it manages everything\nabout the connectivity, and methods on the Prjn handle all the relevant computation.", Fields: []gti.Field{{Name: "LeabraPrj", Doc: "we need a pointer to ourselves as an LeabraPrjn, which can always be used to extract the true underlying type of object when prjn is embedded in other structs -- function receivers do not have this ability so this is necessary."}, {Name: "Off", Doc: "inactivate this projection -- allows for easy experimentation"}, {Name: "Cls", Doc: "Class is for applying parameter styles, can be space separated multple tags"}, {Name: "Notes", Doc: "can record notes about this projection here"}, {Name: "Send", Doc: "sending layer for this projection"}, {Name: "Recv", Doc: "receiving layer for this projection -- the emer.Layer interface can be converted to the specific Layer type you are using, e.g., rlay := prjn.Recv.(*leabra.Layer)"}, {Name: "Pat", Doc: "pattern of connectivity"}, {Name: "Typ", Doc: "type of projection -- Forward, Back, Lateral, or extended type in specialized algorithms -- matches against .Cls parameter styles (e.g., .Back etc)"}, {Name: "RConN", Doc: "number of recv connections for each neuron in the receiving layer, as a flat list"}, {Name: "RConNAvgMax", Doc: "average and maximum number of recv connections in the receiving layer"}, {Name: "RConIndexSt", Doc: "starting index into ConIndex list for each neuron in receiving layer -- just a list incremented by ConN"}, {Name: "RConIndex", Doc: "index of other neuron on sending side of projection, ordered by the receiving layer's order of units as the outer loop (each start is in ConIndexSt), and then by the sending layer's units within that"}, {Name: "RSynIndex", Doc: "index of synaptic state values for each recv unit x connection, for the receiver projection which does not own the synapses, and instead indexes into sender-ordered list"}, {Name: "SConN", Doc: "number of sending connections for each neuron in the sending layer, as a flat list"}, {Name: "SConNAvgMax", Doc: "average and maximum number of sending connections in the sending layer"}, {Name: "SConIndexSt", Doc: "starting index into ConIndex list for each neuron in sending layer -- just a list incremented by ConN"}, {Name: "SConIndex", Doc: "index of other neuron on receiving side of projection, ordered by the sending layer's order of units as the outer loop (each start is in ConIndexSt), and then by the sending layer's units within that"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.Synapse", IDName: "synapse", Doc: "leabra.Synapse holds state for the synaptic connection between neurons", Fields: []gti.Field{{Name: "Wt", Doc: "synaptic weight value -- sigmoid contrast-enhanced"}, {Name: "LWt", Doc: "linear (underlying) weight value -- learns according to the lrate specified in the connection spec -- this is converted into the effective weight value, Wt, via sigmoidal contrast enhancement (see WtSigParams)"}, {Name: "DWt", Doc: "change in synaptic weight, from learning"}, {Name: "Norm", Doc: "DWt normalization factor -- reset to max of abs value of DWt, decays slowly down over time -- serves as an estimate of variance in weight changes over time"}, {Name: "Moment", Doc: "momentum -- time-integrated DWt changes, to accumulate a consistent direction of weight change and cancel out dithering contradictory changes"}, {Name: "Scale", Doc: "scaling parameter for this connection: effective weight value is scaled by this factor -- useful for topographic connectivity patterns e.g., to enforce more distant connections to always be lower in magnitude than closer connections.  Value defaults to 1 (cannot be exactly 0 -- otherwise is automatically reset to 1 -- use a very small number to approximate 0).  Typically set by using the prjn.Pattern Weights() values where appropriate"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.Time", IDName: "time", Doc: "leabra.Time contains all the timing state and parameter information for running a model", Fields: []gti.Field{{Name: "Time", Doc: "accumulated amount of time the network has been running, in simulation-time (not real world time), in seconds"}, {Name: "Cycle", Doc: "cycle counter: number of iterations of activation updating (settling) on the current alpha-cycle (100 msec / 10 Hz) trial -- this counts time sequentially through the entire trial, typically from 0 to 99 cycles"}, {Name: "CycleTot", Doc: "total cycle count -- this increments continuously from whenever it was last reset -- typically this is number of milliseconds in simulation time"}, {Name: "Quarter", Doc: "current gamma-frequency (25 msec / 40 Hz) quarter of alpha-cycle (100 msec / 10 Hz) trial being processed.  Due to 0-based indexing, the first quarter is 0, second is 1, etc -- the plus phase final quarter is 3."}, {Name: "PlusPhase", Doc: "true if this is the plus phase (final quarter = 3) -- else minus phase"}, {Name: "TimePerCyc", Doc: "amount of time to increment per cycle"}, {Name: "CycPerQtr", Doc: "number of cycles per quarter to run -- 25 = standard 100 msec alpha-cycle"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.Quarters", IDName: "quarters", Doc: "Quarters are the different alpha trial quarters, as a bitflag,\nfor use in relevant timing parameters where quarters need to be specified.\nThe Q1..4 defined values are integer *bit positions* -- use Set, Has etc methods\nto set bits from these bit positions."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/leabra/v2/leabra.TimeScales", IDName: "time-scales", Doc: "TimeScales are the different time scales associated with overall simulation running, and\ncan be used to parameterize the updating and control flow of simulations at different scales.\nThe definitions become increasingly subjective imprecise as the time scales increase.\nThis is not used directly in the algorithm code -- all control is responsibility of the\nend simulation.  This list is designed to standardize terminology across simulations and\nestablish a common conceptual framework for time -- it can easily be extended in specific\nsimulations to add needed additional levels, although using one of the existing standard\nvalues is recommended wherever possible."})
